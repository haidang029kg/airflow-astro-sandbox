import logging
import os
import re
from datetime import datetime
from pathlib import Path

import pandas as pd
import xarray as xr

from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

logger = logging.getLogger(__name__)


def _extract_file_name(local_file_path: str):

    return Path(local_file_path).stem


def file_name_to_dt(file_name: str) -> datetime:
    #  Regex pattern to match yyyymmdd format
    pattern = re.compile(r"(\d{4})(\d{2})(\d{2})")
    match = pattern.search(file_name)
    if match:
        year = match.group(1)
        month = match.group(2)
        day = match.group(3)

        # Convert to datetime object
        return datetime.strptime(f"{year}-{month}-{day}", "%Y-%m-%d")
    else:
        raise Exception("does not match file name pattern")


def download_from_s3_task(
    key: str,
    bucket_name: str,
    dest_file_path: str,
):
    logger.info(f"s3 object key in download task file {key}")

    file_path = os.path.join(dest_file_path, key)

    if os.path.exists(file_path):
        logger.info(f"{file_path} does exist, ignore downloading")
        return file_path

    s3_hook = S3Hook(aws_conn_id="aws_default")
    s3_hook.download_file(
        key=key,
        bucket_name=bucket_name,
        local_path=dest_file_path,
        preserve_file_name=True,
        use_autogenerated_subdir=False,
    )
    return file_path


def transform(local_nc_file_path: str) -> pd.DataFrame:
    logger.info("========== open NetCDF data set")
    ds = xr.open_dataset(local_nc_file_path)

    logger.info("========== transform data")
    d_humidity: xr.DataArray = ds.QV
    d_air_temp: xr.DataArray = ds.T

    # humidity transform
    df_humidity = d_humidity.to_dataframe(name="value")
    df_humidity = df_humidity.rename(columns={"value": "humidity"})
    df_humidity["humidity_unit"] = "kg kg-1"
    df_humidity = df_humidity.reset_index()
    logger.info(df_humidity)

    # air temperature transform
    df_air_temp = d_air_temp.to_dataframe(name="value")
    df_air_temp = df_air_temp.rename(columns={"value": "air_temp"})
    df_air_temp["air_temp_unit"] = "K"
    df_air_temp = df_air_temp.reset_index()
    logger.info(df_air_temp)
    # merge
    merged_df = pd.merge(
        df_humidity,
        df_air_temp,
        on=["time", "lev", "lat", "lon"],
    )
    return merged_df


def transform_and_write_to_parquet_task(file_path: str):
    df = transform(file_path)

    file_name = _extract_file_name(file_path)

    parquet_dir = Variable.get("PARQUET_CREATED_DIR")
    local_parquet_file_path = os.path.join(parquet_dir, f"{file_name}.parquet")

    if os.path.exists(local_parquet_file_path):
        os.remove(local_parquet_file_path)
    df.to_parquet(local_parquet_file_path, engine="pyarrow", index=False)

    return local_parquet_file_path


def upload_parquet_to_s3_task(file_path, **kwargs):
    file_name = _extract_file_name(file_path)
    dt = file_name_to_dt(file_name)

    s3_key = f"year={dt.year}/month={dt.month}/date={dt.day}/data.parquet"
    s3_hook = S3Hook(aws_conn_id="aws_default")

    s3_hook.load_file(
        filename=file_path,
        key=s3_key,
        bucket_name=Variable.get("MERRA_DATA_LAKE_BUCKET"),
        replace=True,
    )
    os.remove(file_path)


def get_s3_object_key_from_exec_date(**kwargs):
    execution_date = kwargs["execution_date"]
    logger.info(execution_date)
    formatted_date = execution_date.strftime("%Y%m%d")
    logger.info(f"Formatted execution date: {formatted_date}")

    s3_object_key = f"MERRA2_400.inst3_3d_asm_Np.{formatted_date}.nc4"
    return s3_object_key


with DAG(
    dag_id="merra_etl",
    default_args={
        "owner": "airflow",
        "depends_on_past": False,
        "start_date": datetime(2024, 1, 1),
    },
    description="A DAG to be manually triggered but scheduled daily",
    schedule_interval="@daily",  # Schedule interval set to daily
    catchup=False,
    max_active_runs=1,
) as dag:

    task_execution_date = PythonOperator(
        task_id="get_execution_date_task",
        python_callable=get_s3_object_key_from_exec_date,
        provide_context=True,
    )

    task_download_file_s3 = PythonOperator(
        task_id="download_from_s3_task",
        python_callable=download_from_s3_task,
        provide_context=True,
        op_kwargs={
            "key": "{{ task_instance.xcom_pull(task_ids='get_execution_date_task') }}",
            "bucket_name": Variable.get("MERRA_RAW_BUCKET"),
            "dest_file_path": Variable.get("NC_DOWNLOAD_DIR"),
        },
    )

    task_transform_and_write_parquet = PythonOperator(
        task_id="transform_and_write_to_parquet_task",
        python_callable=transform_and_write_to_parquet_task,
        provide_context=True,  # Ensure context is provided
        op_args=["{{ task_instance.xcom_pull(task_ids='download_from_s3_task') }}"],
    )

    task_upload_parquet_to_s3 = PythonOperator(
        task_id="upload_parquet_to_s3_task",
        python_callable=upload_parquet_to_s3_task,
        op_args=["{{ ti.xcom_pull(task_ids='transform_and_write_to_parquet_task') }}"],
        provide_context=True,
    )

    task_execution_date >> task_download_file_s3 >> task_transform_and_write_parquet >> task_upload_parquet_to_s3  # type: ignore

    """_summary_
airflow dags backfill -s 20240104 -e 20240104 s3_download_dag
airflow dags trigger -e 20240103 s3_download_dag

# Clear all dependent tasks for a specific date
airflow tasks clear -s 20240101 -e 20240104 -r s3_download_dag 
    """
